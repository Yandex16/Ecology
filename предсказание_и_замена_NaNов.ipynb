{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ВАЖНЫЙ КОММЕНТАРИЙ**\n",
        "\n",
        "Обратите внимание:\n",
        "\n",
        "* в датафрейме отсутсвует столбец stage_4_output_danger_gas;\n",
        "\n",
        "* В ходе поспешного изменения, не успел убрать/изменить некоторые обоснования, которые теперь некорректны для данного кода.\n",
        "\n",
        "* значения в DataTime были нормализированы. То есть приведены в диапазон от 0 до 1.\n",
        "\n",
        "* Индесы были сброшены(ресетнуты), из-за чего, например, строка с иднексом 6 стала с индексом 5. Но есть список индексов(до сброса/ресета) **index_series**, который появляется в разделе\n",
        "\"Замена пропусков\". Я так понимаю, что после всех моих строчек кода вам нужно будет изменить индесы датафрема на индексы в этом списке для корректной работы. После этого можно будет так же привести DateTime столбец к типу DateTime аналогичным способом.\n",
        "\n",
        "\n",
        "Сейчас сижу в метро с плохим интернетом. Не могу проверить некоторые изменения, связанные с index_series, поэтому вам придётся принять данную ношу.\n",
        "\n"
      ],
      "metadata": {
        "id": "VA-k5VJmFTu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка данных для предсказания и замены чисел вместо пропусков"
      ],
      "metadata": {
        "id": "BL8JnRqxfAXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импорт библиотек"
      ],
      "metadata": {
        "id": "c-MegJemfGua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install miceforest"
      ],
      "metadata": {
        "id": "oRehfAmwW5ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4eN6ncWzziY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import miceforest as mf\n",
        "from sklearn.base import clone\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from itertools import combinations\n",
        "from scipy.stats import spearmanr, mannwhitneyu\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Важные блоки кода. Функции/Классы"
      ],
      "metadata": {
        "id": "Lj1b5UdnFELo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **функции**"
      ],
      "metadata": {
        "id": "0eOb_oE2e5DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nan_to_median(series: pd.Series):\n",
        "  median_val = series.median()\n",
        "  return series.fillna(median_val)\n",
        "\n",
        "def iqr_filter(df: pd.DataFrame, column: str, lower_bound=True, upper_bound=True, fill_nan=False, multp=3):\n",
        "  \"\"\"\n",
        "  гибкая функция для удаления выбросов с помощью настраиваемого интерквартильного размаха\n",
        "\n",
        "  Аргументы\n",
        "      df: DataFrame содержащий данные, которые нужно отфильтровать.\n",
        "\n",
        "      column: Название столбца в DataFrame df, в которомом будет производиться фильтрация.\n",
        "      Функция будет рассчитывать IQR именно для этого столбца.\n",
        "\n",
        "      lower_bound:  Булевый флаг, определяющий, следует ли отфильтровывать значения,\n",
        "      лежащие ниже нижней границы, рассчитанной на основе IQR. То есть, если\n",
        "      lower_bound = False, то все выбросы(если они есть) будут игнорироваться.\n",
        "\n",
        "      upper_bound: Булевый флаг, определяющий, следует ли отфильтровывать значения,\n",
        "      лежащие выше верхней границы, рассчитанной на основе IQR.\n",
        "\n",
        "      multp: Множитель, используемый для расчета границ фильтрации.\n",
        "  \"\"\"\n",
        "  df_copy = df.copy()\n",
        "\n",
        "  if fill_nan:\n",
        "    df_copy[column] = nan_to_median(df_copy[column])\n",
        "\n",
        "  q1, q3 = np.percentile(df_copy[column], [25, 75])\n",
        "  iqr = (q3 - q1) * multp\n",
        "  low_bound = q1 - iqr\n",
        "  up_bound = q3 + iqr\n",
        "\n",
        "  median = 0 # медиана\n",
        "  outlines = 0 # индексы, значения строк столбца которых необходимо заменить медианой\n",
        "\n",
        "  # в этих ситуациях мы ищем медиану чисел, которые не считаем за выбросы (1)\n",
        "  # то есть мы считаем за выбросы up_bound или/и low_bound и не используем их диапазон значений\n",
        "  # для поиска медианы.\n",
        "  if lower_bound and upper_bound:\n",
        "    outlines = df_copy[(df_copy[column] < low_bound) | (df_copy[column] > up_bound)].index\n",
        "    median = df_copy[(df_copy[column] >= low_bound) & (df_copy[column] <= up_bound)][column].median() # та самая медиана (1)\n",
        "  elif lower_bound:\n",
        "    outlines = df_copy[df_copy[column] < low_bound].index\n",
        "    median = df_copy[df_copy[column] >= low_bound][column].median() # та самая медиана (1)\n",
        "  elif upper_bound:\n",
        "    outlines = df_copy[df_copy[column] > up_bound].index\n",
        "    median = df_copy[df_copy[column] <= up_bound][column].median() # та самая медиана (1)\n",
        "\n",
        "  df.loc[outlines, column] = median\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "def find_dependencies(correlation_matrix, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Выводит пары зависимых признаков на основе матрицы корреляции.\n",
        "\n",
        "    Аргументы:\n",
        "        correlation_matrix: Pandas DataFrame с матрицей корреляции.\n",
        "        threshold: Порог корреляции для определения зависимости.\n",
        "    \"\"\"\n",
        "\n",
        "    dependent_features = {} # словарь для хранения выявленных зависимостей.\n",
        "\n",
        "    for feature in correlation_matrix.columns:  # цикл переберает все столбцы в матрице\n",
        "        correlations = correlation_matrix[feature] # для текущей feature эта строка извлекает\n",
        "                                                   # значения корреляции этой функции со всеми\n",
        "                                                   # остальными функциями в виде pd.Series .\n",
        "        dependencies = correlations[correlations.abs() >= threshold].index.tolist() # основная логика\n",
        "        #correlations.abs() >= threshold: Это создаёт логическую маску, которая выбирает только\n",
        "        #значения корреляции, которые по модулю абсолютного значения больше или равны threshold.\n",
        "        #.index.tolist(): .index извлекает индексы (имена признаков), соответствующие\n",
        "        #выбранным значениям корреляции, и .tolist() преобразует эти индексы в список.\n",
        "\n",
        "\n",
        "        if dependencies:  # если есть зависимости\n",
        "          dependencies.remove(feature) # удаляем зависимость с самим собой\n",
        "          dependent_features[feature] = dependencies # выявленные зависимости сохраняются\n",
        "                                                     # в словаре dependent_features.\n",
        "                                                     # Текущий признак становится ключом,\n",
        "                                                     # а список его зависимостей — значением.\n",
        "    return dependent_features # пары зависимых признаков на основе матрицы корреляции.\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "def visualize_correlation_matrix(correlation_matrix, title=\"The best graphic\"): # красивый график ВСЕХ корреляций(колличественных)\n",
        "    \"\"\"\n",
        "    Рисует график корреляций всех колличественных признаков(и не только, если надо).\n",
        "\n",
        "    Аругменты:\n",
        "        correlation_matrix:  Фрейм данных Pandas, представляющий корреляционную матрицу.\n",
        "        threshold: Название графика.\n",
        "    \"\"\"\n",
        "    for col in correlation_matrix.columns: # цикл повторяется по каждому столбцу корреляционной матрицы.\n",
        "        correlation_matrix[col] = pd.to_numeric(correlation_matrix[col], errors='coerce') # спасает от вылета программы.\n",
        "\n",
        "    #correlation_matrix_np = correlation_matrix.values # преобразует фрейм данных Pandas в массив NumPy\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest') # создание тепловой карты корреляций\n",
        "\n",
        "    plt.colorbar(label='Correlation')\n",
        "\n",
        "    plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
        "    plt.yticks(range(len(correlation_matrix.index)), correlation_matrix.index)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "def super_train_test_split(df: pd.DataFrame, y: pd.Series):\n",
        "  '''\n",
        "  Делит данные на две выборки: 1. строки, значения необходимого нам столбца не имеют пропусков.\n",
        "                               2. строки, значения необходимого нам столбца имеют пропуски.\n",
        "  Каждый из этих пунктов так же делиться на две выборки: а) необходимый столбец.\n",
        "                                                         б) остальные факторы.\n",
        "\n",
        "  Аргументы:\n",
        "    df: Pandas DataFrame, состоящий из факторов, инмеющих зависимость с признаком,\n",
        "        в котором необходимо заполнить пропуски.\n",
        "\n",
        "    y: Pandas Series, признак, пропуски которого необходимо заполнить.\n",
        "\n",
        "  небольшой комментарий:\n",
        "  У нас есть проблема - для заполенния пропусков с помощью какой-либо модели, необходимо,\n",
        "  чтобы ВСЕ значения в других признаках были заполнены(не было пропусков).\n",
        "  В противном случае модель ругается, что есть NaNы. Данный цикл устраняет данную проблему,\n",
        "  временно заполняя пропуски в столбцах на медиану всех значений признака\n",
        "  (кроме столбца, задача для которого изначально была заполнить пропуски с помощью модели).\n",
        "  Дальше смотрите по комментариям\n",
        "  '''\n",
        "  X = df.copy()\n",
        "\n",
        "  for col in X.columns: # временно заменяем пропуски в зависимых факторах на медиану\n",
        "    if X[col].isnull().any() == True:\n",
        "      median_value = X[col].median()\n",
        "      X[col] = X[col].fillna(median_value)\n",
        "\n",
        "\n",
        "  y_train = y[y.isnull() == False] # отбираем для тренировки те строки, в которых присутсвуют данные\n",
        "  y_temp = y[y.isnull()] # просто мусор. Полезный\n",
        "\n",
        "  idxs = y_temp.index # берём иднексы мусора(индексы,\n",
        "                      # в строках которых есть пропуски, которые необходимо заполнить)\n",
        "  X_train = X.drop(idxs) # делаем обучающую выборку из строк, в которых нет пропусков\n",
        "\n",
        "  idxs = y_train.index # берём иднексы c изначально заполенными значениями\n",
        "  X_test = X.drop(idxs) # отбрасываем строки с заполненными значениями в нужном нам столбце.\n",
        "                        # Получается выборка с данными, на основе которых будут\n",
        "                        # предсказываться пропущенные значения\n",
        "\n",
        "  return X_train, X_test, y_train, y_temp\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "def split_for_grade(df: pd.DataFrame, target_column: pd.Series): # без комментариев, это вам не пригодится\n",
        "  X = df.copy()\n",
        "\n",
        "  y = target_column\n",
        "  y1 = y[y.isnull() == False] # отбираем для тренировки те строки, в которых присутсвуют данные\n",
        "  y_temp = y[y.isnull()] # просто мусор. Полезный\n",
        "\n",
        "  idxs = y_temp.index # берём иднексы мусора(индексы,\n",
        "                      # в строках которых есть пропуски, которые необходимо заполнить)\n",
        "  X = X.drop(idxs) # делаем обучающую выборку из строк, в которых нет пропусков\n",
        "  for col in X.columns:\n",
        "    if X[col].isnull().any() == True:\n",
        "      median_value = X[col].median()\n",
        "      X[col] = X[col].fillna(median_value)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y1, test_size=len(y_temp) / len(y1), random_state=42)\n",
        "  return X_train.values, X_test.values, y_train.values, y_test.values"
      ],
      "metadata": {
        "id": "qoOYkOrucjg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Классы***"
      ],
      "metadata": {
        "id": "NfJGfecwQGAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SBS():\n",
        "    \"\"\"\n",
        "    Класс для последовательного обратного отбора признаков (Sequential Backward Selection).\n",
        "\n",
        "    Алгоритм отбирает подмножество наиболее важных признаков,\n",
        "    оптимизируя модель по метрикам качества (R-квадрат и MSE).\n",
        "\n",
        "    Аргументы:\n",
        "        estimator: Модель машинного обучения, которую нужно оптимизировать.\n",
        "                   Должна поддерживать методы fit и predict.\n",
        "        k_features: Целевое количество признаков для отбора.\n",
        "        test_size: Доля данных для тестирования (кросс-валидация).\n",
        "        random_state: Случайное зерно для воспроизводимости результатов.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, estimator, k_features,\n",
        "                test_size=0.25, random_state=42):\n",
        "        self.estimator = clone(estimator) # Создаём копию модели, чтобы не менять исходную\n",
        "        self.k_features = k_features\n",
        "        self.test_size = test_size\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y, own_split=False):\n",
        "        \"\"\"\n",
        "        Обучает модель SBS и отбирает лучшие признаки.\n",
        "\n",
        "        Args:\n",
        "            X: Матрица признаков.\n",
        "            y: Вектор целевой переменной.\n",
        "            own_split: Если True, использует пользовательскую функцию split_for_grade для разделения данных.\n",
        "\n",
        "        Returns:\n",
        "            Возвращает себя (self) для цепочки вызовов.\n",
        "        \"\"\"\n",
        "\n",
        "        # Разделение данных на обучающую и тестовую выборки\n",
        "        X_train, X_test, y_train, y_test = split_for_grade(X, y)\n",
        "\n",
        "        dim = X_train.shape[1]\n",
        "        self.indices_ = tuple(range(dim))  # Индексы всех признаков\n",
        "        self.subsets_ = [self.indices_] # Список всех подмножеств признаков\n",
        "\n",
        "        # Вычисляем R-SQUARED и MSE\n",
        "        score = self._calc_score(X_train, y_train,\n",
        "                                    X_test, y_test, self.indices_)\n",
        "        self.scores_ = [score]\n",
        "\n",
        "        while dim > self.k_features:\n",
        "            scores = []\n",
        "            subsets = []\n",
        "\n",
        "            # Перебор всех возможных подмножеств с одним удаленным признаком\n",
        "            for p in combinations(self.indices_, r=dim - 1):\n",
        "                score = self._calc_score(X_train, y_train,\n",
        "                                            X_test, y_test, p)\n",
        "                scores.append(score)\n",
        "                subsets.append(p)\n",
        "\n",
        "            # находим подмножества с лучшими значениями метрик\n",
        "            best = np.argmax([i[0] for i in scores]) #  Выбираем подмножество с наибольшим r2_score,\n",
        "                                                     # т.к. данная метрика в приоритете. Так же отбор\n",
        "                                                     # лучшей комбинации будет происходит вне класса\n",
        "            self.indices_ = subsets[best]\n",
        "            self.subsets_.append(self.indices_)\n",
        "            dim -= 1\n",
        "\n",
        "            self.scores_.append(scores[best])\n",
        "        self.k_score_ = self.scores_[-1]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Возвращает матрицу признаков с отобранными признаками.\n",
        "\n",
        "        Аргументы:\n",
        "            X: Матрица признаков.\n",
        "        \"\"\"\n",
        "        return X[:, self.indices_]\n",
        "\n",
        "    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n",
        "        \"\"\"\n",
        "        Вычисляет метрики R-SQUARED и MSE для заданного подмножества признаков.\n",
        "\n",
        "        Аргументы:\n",
        "            X_train, y_train, X_test, y_test, indices: Данные для обучения и оценки модели.\n",
        "        \"\"\"\n",
        "        self.estimator.fit(X_train[:, indices], y_train)\n",
        "        y_pred = self.estimator.predict(X_test[:, indices])\n",
        "\n",
        "        score = [r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred)]\n",
        "        return score\n"
      ],
      "metadata": {
        "id": "zX-fdh-AQLD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Инициализация моделей и главного дата фрейма"
      ],
      "metadata": {
        "id": "TnsaRLc2TLur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "инициализация регрессионных моделей и т.п."
      ],
      "metadata": {
        "id": "he0gIoTCE5im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_model = RandomForestRegressor(random_state=42)\n",
        "linear_regression = LinearRegression()\n",
        "knn_model = KNeighborsRegressor(n_neighbors=3)\n",
        "scaler = MinMaxScaler()"
      ],
      "metadata": {
        "id": "bzyK1PAyrxcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_environmental_data = pd.read_csv(\"analysing_environmental_issues.csv\", sep=',') # главный датафрейм"
      ],
      "metadata": {
        "id": "OUuduy4i3Sqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_environmental_data"
      ],
      "metadata": {
        "id": "2rgP6v2yJPbW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## предобработка перед оценкой моделей и предсказыванием значений"
      ],
      "metadata": {
        "id": "oCeMVlpIGV7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Просто копия главного датафрейма для экспериментов"
      ],
      "metadata": {
        "id": "bZrv9db_Jk7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_environmental_data.copy()"
      ],
      "metadata": {
        "id": "S84pSB0E3MpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Регрессионным моделям не нравится столбец DateTime, поэтому я его пока что просто удалил, но можно поставить в качестве индекса. DateTime сам по себе не очень полезен(на этапе предобработки).   "
      ],
      "metadata": {
        "id": "rTzHo9yRKf5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['DateTime'] = pd.to_datetime(df['DateTime'], errors='coerce')\n",
        "time_diffs = df['DateTime'].diff().dt.total_seconds()\n",
        "time_diffs = time_diffs.fillna(0)\n",
        "\n",
        "# нормализуем даты из столбца DateTime\n",
        "normalized_diffs = scaler.fit_transform(time_diffs.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# вычисляет кумулятивную сумму элементов\n",
        "normalized_times = np.cumsum(normalized_diffs)\n",
        "\n",
        "# подставляем нормализованные значение\n",
        "df['DateTime'] = normalized_times\n",
        "\n",
        "df.pop(\"stage_4_output_danger_gas\") # значений мало, признак на данном этапе бесполезный.\n",
        "\n",
        "df = df.drop_duplicates(subset=df.columns[1:], keep=False) # удаляем все дупликаты.\n",
        "# Очень важно, что дубликаты будут найдены, если игнорировать столбец \"DateTime\".\n",
        "df"
      ],
      "metadata": {
        "id": "W5S7rikVK5-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# удаляем супер выбросы/критические ошибки, так как такие значения\n",
        "# могут непропорционально влиять на оценку параметров модели и\n",
        "# вносить нестабильность в обучение модели. Для этого используем\n",
        "# тройной межквартильный размах\n",
        "for col in df.columns[1:-1]:\n",
        "  iqr_filter(df, col, fill_nan=True, multp=3)\n",
        "\n",
        "#df.describe()"
      ],
      "metadata": {
        "id": "Qlx71DJDeZ9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Было протестированно множество разных моделей. В конченом счёте лучшими моделями оказались KNN и Random Trees Forest. В ходе размышлений было принято использовать KNN для тех признаков, если метрики R-SQUAED и MSE показали хорошие значения в тестовых выборках. Для отбора признаков в зависимости от которых будут предсказываться значения на месте пропусков в отклике."
      ],
      "metadata": {
        "id": "KYXrwAIh4Rio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SBS  для выбора факторов , на основе которых буддет обучаться модель и предсказываться значения**"
      ],
      "metadata": {
        "id": "6U8Q5OYLuoDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNeighborsRegressor**"
      ],
      "metadata": {
        "id": "9e6XG6X81SGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_dict_NaN = {}"
      ],
      "metadata": {
        "id": "84Qn0Qe1KvkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbs = SBS(knn_model, k_features=1)\n",
        "\n",
        "for column in df.columns:\n",
        "    if df[column].isnull().any() == True:\n",
        "        y = df[column]\n",
        "        X = df.copy()\n",
        "        X.pop(column)\n",
        "\n",
        "        new_names = [i for i in range(len(df.columns))]\n",
        "        X = X.rename(columns=dict(zip(X, new_names)))\n",
        "        sbs.fit(X, y, own_split=True)\n",
        "\n",
        "        k_feat = []\n",
        "        r2 = []\n",
        "        mse = []\n",
        "\n",
        "\n",
        "        for scores in sbs.scores_:\n",
        "            if scores[1] < 10 and scores[0] > 0.7:\n",
        "                k_feat.append(len(sbs.subsets_[sbs.scores_.index(scores)]))\n",
        "                r2.append(scores[0])\n",
        "                mse.append(scores[1])\n",
        "\n",
        "        if len(mse) > 0:\n",
        "\n",
        "            r2_threshold = 0.7\n",
        "            mse_threshold = 10\n",
        "\n",
        "            # Normalize MSE values\n",
        "            mse_values = np.array([item[1] for item in sbs.scores_])\n",
        "            scaler = MinMaxScaler()\n",
        "            mse_values_normalized = scaler.fit_transform(mse_values.reshape(-1, 1)).flatten()\n",
        "\n",
        "            best_r2 = -1\n",
        "            best_mse = float('inf')\n",
        "            best_pair = None\n",
        "            lk = -1\n",
        "            for i, (r2_sc, mse_sc) in enumerate(sbs.scores_):\n",
        "                normalized_mse = mse_values_normalized[i]\n",
        "                rounded_mse = round(normalized_mse, 3)\n",
        "\n",
        "                if r2_sc > r2_threshold and mse_sc <= mse_threshold:\n",
        "                    if r2_sc > best_r2 and rounded_mse < best_mse:\n",
        "                        best_r2 = r2_sc\n",
        "                        best_mse = rounded_mse\n",
        "                        best_pair = [r2_sc, mse_sc]\n",
        "                        lk = list(sbs.subsets_[sbs.scores_.index([r2_sc, mse_sc])])\n",
        "\n",
        "            print(f\"Best count: {len(lk)}\")\n",
        "            print(f\"Best pair: R2 = {best_pair[0]:.4f}, MSE = {best_pair[1]:.4f}, Normalized MSE = {best_mse:.4f}\")\n",
        "\n",
        "            columns_dict_NaN[column] = list(df.columns[0:][lk])\n",
        "            #dk[column] = X.columns[1:, list(sbs.subsets_[10])]\n",
        "\n",
        "            plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
        "\n",
        "            # Plot R-squared\n",
        "            plt.plot(k_feat, r2, marker='o', linestyle='-', label='R-squared')\n",
        "\n",
        "            # Plot MSE\n",
        "            plt.plot(k_feat, mse, marker='x', linestyle='-', label='MSE')\n",
        "\n",
        "            plt.xlabel(\"Number of Features\")\n",
        "            plt.ylabel(\"Metric Value\")\n",
        "            plt.title(column)\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "print(columns_dict_NaN)"
      ],
      "metadata": {
        "id": "Ie-dIER4YeRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbs = SBS(random_forest_model, k_features=1)\n",
        "\n",
        "for column in df.columns:\n",
        "    if df[column].isnull().any() == True:\n",
        "        y = df[column]\n",
        "        X = df.copy()\n",
        "        X.pop(column)\n",
        "\n",
        "        new_names = [i for i in range(len(df.columns))]\n",
        "        X = X.rename(columns=dict(zip(X, new_names)))\n",
        "        sbs.fit(X, y, own_split=True)\n",
        "\n",
        "        k_feat = []\n",
        "        r2 = []\n",
        "        mse = []\n",
        "\n",
        "\n",
        "        for scores in sbs.scores_:\n",
        "            if scores[1] < 10 and scores[0] > 0.7:\n",
        "                k_feat.append(len(sbs.subsets_[sbs.scores_.index(scores)]))\n",
        "                r2.append(scores[0])\n",
        "                mse.append(scores[1])\n",
        "\n",
        "        if len(mse) > 0:\n",
        "\n",
        "            r2_threshold = 0.7\n",
        "            mse_threshold = 10\n",
        "\n",
        "            # Normalize MSE values\n",
        "            mse_values = np.array([item[1] for item in sbs.scores_])\n",
        "            scaler = MinMaxScaler()\n",
        "            mse_values_normalized = scaler.fit_transform(mse_values.reshape(-1, 1)).flatten()\n",
        "\n",
        "            best_r2 = -1\n",
        "            best_mse = float('inf')\n",
        "            best_pair = None\n",
        "            lk = -1\n",
        "            for i, (r2_sc, mse_sc) in enumerate(sbs.scores_):\n",
        "                normalized_mse = mse_values_normalized[i]\n",
        "                rounded_mse = round(normalized_mse, 3)\n",
        "\n",
        "                if r2_sc > r2_threshold and mse_sc <= mse_threshold:\n",
        "                    if r2_sc > best_r2 and rounded_mse < best_mse:\n",
        "                        best_r2 = r2_sc\n",
        "                        best_mse = rounded_mse\n",
        "                        best_pair = [r2_sc, mse_sc]\n",
        "                        lk = list(sbs.subsets_[sbs.scores_.index([r2_sc, mse_sc])])\n",
        "\n",
        "            print(f\"Best count: {len(lk)}\")\n",
        "            print(f\"Best pair: R2 = {best_pair[0]:.4f}, MSE = {best_pair[1]:.4f}, Normalized MSE = {best_mse:.4f}\")\n",
        "\n",
        "            columns_dict_NaN[column] = list(df.columns[0:][lk])\n",
        "\n",
        "\n",
        "print(columns_dict_NaN)"
      ],
      "metadata": {
        "id": "_VoJkOXWeSj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(columns_dict_NaN.keys()))"
      ],
      "metadata": {
        "id": "tcF4gBhfZV_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Замена пропусков"
      ],
      "metadata": {
        "id": "I569V-8GvzLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "db5UHjRUdF5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "главный гость нашей программы. Весь код ранее был нужен для корректной работы этих строчек кода.\n",
        "Здесь мы заменяем NaNы на прогнозируемые значения"
      ],
      "metadata": {
        "id": "JL-ltfo8Gaqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column in columns_dict_NaN.keys(): # для каждого столбца в котором остались пропущенные значения\n",
        "    print(column)\n",
        "    X = df.loc[:, columns_dict_NaN[column]].copy() # все зависимые признаки с столбцом column ()\n",
        "    y = df[column] # наш столбец.\n",
        "\n",
        "    X_train, X_test, y_train, y_temp = super_train_test_split(X, y) # Делим наши данные на\n",
        "                                                                    # на обучающую и тестовую выборку\n",
        "    knn_model.fit(X_train, y_train) # обучаем модель knn(для каждого столбца)\n",
        "\n",
        "    y_pred = knn_model.predict(X_test) # предсказываем пропущенные значения\n",
        "\n",
        "    df.loc[X_test.index, column] = y_pred # вставляем предсказания на места пропущенных значений"
      ],
      "metadata": {
        "id": "jG0YPweEUftH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_list = df.index.tolist()\n",
        "index_series = pd.Series(index_list)\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "dTkUd832b6ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel = mf.ImputationKernel(\n",
        "    data=df, #Useful for convergence analysis.\n",
        "    random_state=42  #For reproducibility\n",
        ")\n",
        "\n",
        "# Perform MICE imputation. Experiment with 'iterations'.  Too few might not converge,\n",
        "# too many might be computationally expensive with diminishing returns.\n",
        "kernel.mice(iterations=10) # Number of iterations\n",
        "\n",
        "# Get the completed datasets\n",
        "df = kernel.complete_data()\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "Yvn7S7xeiwAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe() # просто смотрю на изменение данных"
      ],
      "metadata": {
        "id": "oqvxm5pZH6bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "QErSW097n-uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Как интегрировать это чюдо в свой код"
      ],
      "metadata": {
        "id": "rxw2YDiYLnoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0nVUUORqMQcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Насколько я понимаю, вам достаточно скопировать всё эти махинации в самое начало вашего блокнота и после последней строчки моего кода написать\n",
        "\"название вашей переменной с датафреймом\" = df\n"
      ],
      "metadata": {
        "id": "iPCEMrolLDw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#my_data = df"
      ],
      "metadata": {
        "id": "D-CsLmI3LfQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "А как буд-то мы можем себя возомнить гениями и в итоговом проекте сделать отдельный скрипт с этим кодом, который будет записывать в файлик csv Датафрейм без пропусков(или использовать данный скрипт как библиотеку и написать здесь функцию, которая будет возвращать обработанный дата фрейм), чтобы не загромождать один файл кодом."
      ],
      "metadata": {
        "id": "nciUrX5SL2FI"
      }
    }
  ]
}