{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA-k5VJmFTu-"
      },
      "source": [
        "**ВАЖНЫЙ КОММЕНТАРИЙ**\n",
        "\n",
        "Обратите внимание:\n",
        "\n",
        "* признак **work_shift** не изменялся\n",
        "\n",
        "* супер выбросы удалил с помощью метода тройного интерквартильного размаха. (для этого была написана функция iqr_filter)\n",
        "\n",
        "* Во время работы с датафреймом были сброшены индексы, из-за чего некоторые строки встали на иные индексы. Также значения столбца DateTime были изменены на числа в диапазоне от 0 до 1. **НО** данные проблемы я вроде как исправил(поставил строки на прежние индексы и перевёл значения столбца DateTime в тип datetime), поэтому проблем с этим быть не должно, если просто продолжить работать с датафреймом\n",
        "\n",
        "* stage_4_output_danger_gas я тоже удалял, но добавил с учётом индексов. Вроде проблем вызвать не должен\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL8JnRqxfAXL"
      },
      "source": [
        "# Подготовка данных для предсказания и замены чисел вместо пропусков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-MegJemfGua"
      },
      "source": [
        "## Импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oRehfAmwW5ZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a4d831-fecc-45f1-c16f-951b565947ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: miceforest in /usr/local/lib/python3.10/dist-packages (6.0.3)\n",
            "Requirement already satisfied: lightgbm>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from miceforest) (4.5.0)\n",
            "Requirement already satisfied: pandas>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from miceforest) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from miceforest) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from miceforest) (1.13.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from miceforest) (17.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->miceforest) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->miceforest) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->miceforest) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->miceforest) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install miceforest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G4eN6ncWzziY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import miceforest as mf\n",
        "from sklearn.base import clone\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from itertools import combinations\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj1b5UdnFELo"
      },
      "source": [
        "## Важные блоки кода. Функции/Классы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eOb_oE2e5DF"
      },
      "source": [
        "### **функции**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qoOYkOrucjg1"
      },
      "outputs": [],
      "source": [
        "def nan_to_median(series: pd.Series):\n",
        "  median_val = series.median()\n",
        "  return series.fillna(median_val)\n",
        "\n",
        "def iqr_filter(df: pd.DataFrame, column: str, lower_bound=True, upper_bound=True, fill_nan=False, multp=3):\n",
        "  \"\"\"\n",
        "  гибкая функция для замены выбросов медианой с помощью настраиваемого интерквартильного размаха\n",
        "\n",
        "  Аргументы\n",
        "      df: DataFrame содержащий данные, которые нужно отфильтровать.\n",
        "\n",
        "      column: Название столбца в DataFrame df, в которомом будет производиться фильтрация.\n",
        "      Функция будет рассчитывать IQR именно для этого столбца.\n",
        "\n",
        "      lower_bound:  Булевый флаг, определяющий, следует ли отфильтровывать значения,\n",
        "      лежащие ниже нижней границы, рассчитанной на основе IQR. То есть, если\n",
        "      lower_bound = False, то все выбросы(если они есть) будут игнорироваться.\n",
        "\n",
        "      upper_bound: Булевый флаг, определяющий, следует ли отфильтровывать значения,\n",
        "      лежащие выше верхней границы, рассчитанной на основе IQR.\n",
        "\n",
        "      multp: Множитель, используемый для расчета границ фильтрации.\n",
        "  \"\"\"\n",
        "  df_copy = df.copy()\n",
        "\n",
        "  if fill_nan:\n",
        "    df_copy[column] = nan_to_median(df_copy[column])\n",
        "\n",
        "  q1, q3 = np.percentile(df_copy[column], [25, 75])\n",
        "  iqr = (q3 - q1) * multp\n",
        "  low_bound = q1 - iqr\n",
        "  up_bound = q3 + iqr\n",
        "\n",
        "  median = 0 # медиана\n",
        "  outlines = 0 # индексы, значения строк столбца которых необходимо заменить медианой\n",
        "\n",
        "  # в этих ситуациях мы ищем медиану среди чисел, которые не считаем за выбросы (1)\n",
        "  # то есть мы считаем за выбросы up_bound или/и low_bound и не используем их диапазон значений\n",
        "  # для поиска медианы.\n",
        "  if lower_bound and upper_bound:\n",
        "    outlines = df_copy[(df_copy[column] < low_bound) | (df_copy[column] > up_bound)].index\n",
        "    median = df_copy[(df_copy[column] >= low_bound) & (df_copy[column] <= up_bound)][column].median() # та самая медиана (1)\n",
        "  elif lower_bound:\n",
        "    outlines = df_copy[df_copy[column] < low_bound].index\n",
        "    median = df_copy[df_copy[column] >= low_bound][column].median() # та самая медиана (1)\n",
        "  elif upper_bound:\n",
        "    outlines = df_copy[df_copy[column] > up_bound].index\n",
        "    median = df_copy[df_copy[column] <= up_bound][column].median() # та самая медиана (1)\n",
        "\n",
        "  df.loc[outlines, column] = np.nan\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "def radical_iqr_filter(df: pd.DataFrame, column: str, lower_bound=True, upper_bound=True, fill_nan=False, multp=3):\n",
        "  \"\"\"\n",
        "  гибкая функция замены выбросов на NaN с помощью настраиваемого интерквартильного размаха\n",
        "\n",
        "  Аргументы\n",
        "      df: DataFrame содержащий данные, которые нужно отфильтровать.\n",
        "\n",
        "      column: Название столбца в DataFrame df, в которомом будет производиться фильтрация.\n",
        "      Функция будет рассчитывать IQR именно для этого столбца.\n",
        "\n",
        "      lower_bound:  Булевый флаг, определяющий, следует ли отфильтровывать значения,\n",
        "      лежащие ниже нижней границы, рассчитанной на основе IQR. То есть, если\n",
        "      lower_bound = False, то все выбросы(если они есть) будут игнорироваться.\n",
        "\n",
        "      upper_bound: Булевый флаг, определяющий, следует ли отфильтровывать значения,\n",
        "      лежащие выше верхней границы, рассчитанной на основе IQR.\n",
        "\n",
        "      multp: Множитель, используемый для расчета границ фильтрации.\n",
        "  \"\"\"\n",
        "  df_copy = df.copy()\n",
        "\n",
        "  if fill_nan:\n",
        "    df_copy[column] = nan_to_median(df_copy[column])\n",
        "\n",
        "  q1, q3 = np.percentile(df_copy[column], [25, 75])\n",
        "  iqr = (q3 - q1) * multp\n",
        "  low_bound = q1 - iqr\n",
        "  up_bound = q3 + iqr\n",
        "\n",
        "  median = 0 # медиана\n",
        "  outlines = 0 # индексы, значения строк столбца которых необходимо заменить медианой\n",
        "\n",
        "  # в этих ситуациях мы ищем медиану среди чисел, которые не считаем за выбросы (1)\n",
        "  # то есть мы считаем за выбросы up_bound или/и low_bound и не используем их диапазон значений\n",
        "  # для поиска медианы.\n",
        "  if lower_bound and upper_bound:\n",
        "    outlines = df_copy[(df_copy[column] < low_bound) | (df_copy[column] > up_bound)].index\n",
        "    median = df_copy[(df_copy[column] >= low_bound) & (df_copy[column] <= up_bound)][column].median() # та самая медиана (1)\n",
        "  elif lower_bound:\n",
        "    outlines = df_copy[df_copy[column] < low_bound].index\n",
        "    median = df_copy[df_copy[column] >= low_bound][column].median() # та самая медиана (1)\n",
        "  elif upper_bound:\n",
        "    outlines = df_copy[df_copy[column] > up_bound].index\n",
        "    median = df_copy[df_copy[column] <= up_bound][column].median() # та самая медиана (1)\n",
        "\n",
        "  df.loc[outlines, column] = median\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "def super_train_test_split(df: pd.DataFrame, y: pd.Series):\n",
        "  '''\n",
        "  Делит данные на две выборки: 1. строки, значения необходимого нам столбца не имеют пропусков.\n",
        "                               2. строки, значения необходимого нам столбца имеют пропуски.\n",
        "  Каждый из этих пунктов так же делиться на две выборки: а) необходимый столбец.\n",
        "                                                         б) остальные факторы.\n",
        "\n",
        "  Аргументы:\n",
        "    df: Pandas DataFrame, состоящий из факторов, инмеющих зависимость с признаком,\n",
        "        в котором необходимо заполнить пропуски.\n",
        "\n",
        "    y: Pandas Series, признак, пропуски которого необходимо заполнить.\n",
        "\n",
        "  небольшой комментарий:\n",
        "  У нас есть проблема - для заполенния пропусков с помощью какой-либо модели, необходимо,\n",
        "  чтобы ВСЕ значения в других признаках были заполнены(не было пропусков).\n",
        "  В противном случае модель ругается, что есть NaNы. Данный цикл устраняет данную проблему,\n",
        "  временно заполняя пропуски в столбцах на медиану всех значений признака\n",
        "  (кроме столбца, задача для которого изначально была заполнить пропуски с помощью модели).\n",
        "  Дальше смотрите по комментариям\n",
        "  '''\n",
        "  X = df.copy()\n",
        "\n",
        "  for col in X.columns: # временно заменяем пропуски в зависимых факторах на медиану\n",
        "    if X[col].isnull().any() == True:\n",
        "      median_value = X[col].median()\n",
        "      X[col] = X[col].fillna(median_value)\n",
        "\n",
        "\n",
        "  y_train = y[y.isnull() == False] # отбираем для тренировки те строки, в которых присутсвуют данные\n",
        "  y_temp = y[y.isnull()] # просто мусор. Полезный\n",
        "\n",
        "  idxs = y_temp.index # берём иднексы мусора(индексы,\n",
        "                      # в строках которых есть пропуски, которые необходимо заполнить)\n",
        "  X_train = X.drop(idxs) # делаем обучающую выборку из строк, в которых нет пропусков\n",
        "\n",
        "  idxs = y_train.index # берём иднексы c изначально заполенными значениями\n",
        "  X_test = X.drop(idxs) # отбрасываем строки с заполненными значениями в нужном нам столбце.\n",
        "                        # Получается выборка с данными, на основе которых будут\n",
        "                        # предсказываться пропущенные значения\n",
        "\n",
        "  return X_train, X_test, y_train, y_temp\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "def split_for_grade(df: pd.DataFrame, target_column: pd.Series): # просто раздел данных на\n",
        "                                                                 # выборки для обучения и тестирования\n",
        "  X = df.copy()\n",
        "\n",
        "  y = target_column\n",
        "  y1 = y[y.isnull() == False] # отбираем для тренировки те строки, в которых присутсвуют данные\n",
        "  y_temp = y[y.isnull()] # просто мусор. Полезный\n",
        "\n",
        "  idxs = y_temp.index # берём иднексы мусора(индексы,\n",
        "                      # в строках которых есть пропуски, которые необходимо заполнить)\n",
        "  X = X.drop(idxs) # делаем обучающую выборку из строк, в которых нет пропусков\n",
        "  for col in X.columns:\n",
        "    if X[col].isnull().any() == True:\n",
        "      median_value = X[col].median()\n",
        "      X[col] = X[col].fillna(median_value)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y1, test_size=len(y_temp) / len(y1), random_state=42)\n",
        "  return X_train.values, X_test.values, y_train.values, y_test.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfJGfecwQGAV"
      },
      "source": [
        "### **Классы**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zX-fdh-AQLD5"
      },
      "outputs": [],
      "source": [
        "class SBS():\n",
        "    \"\"\"\n",
        "    Класс для последовательного обратного отбора признаков (Sequential Backward Selection).\n",
        "\n",
        "    Алгоритм отбирает подмножество наиболее важных признаков,\n",
        "    оптимизируя модель по метрикам качества (R-квадрат и MSE).\n",
        "\n",
        "    Аргументы:\n",
        "        estimator: Модель машинного обучения, которую нужно оптимизировать.\n",
        "                   Должна поддерживать методы fit и predict.\n",
        "        k_features: Целевое количество признаков для отбора.\n",
        "        test_size: Доля данных для тестирования (кросс-валидация).\n",
        "        random_state: Случайное зерно для воспроизводимости результатов.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, estimator, k_features,\n",
        "                test_size=0.25, random_state=42):\n",
        "        self.estimator = clone(estimator) # Создаём копию модели, чтобы не менять исходную\n",
        "        self.k_features = k_features\n",
        "        self.test_size = test_size\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y, own_split=False):\n",
        "        \"\"\"\n",
        "        Обучает модель SBS и отбирает лучшие признаки.\n",
        "\n",
        "        Args:\n",
        "            X: Матрица признаков.\n",
        "            y: Вектор целевой переменной.\n",
        "            own_split: Если True, использует пользовательскую функцию split_for_grade для разделения данных.\n",
        "\n",
        "        Returns:\n",
        "            Возвращает себя (self) для цепочки вызовов.\n",
        "        \"\"\"\n",
        "\n",
        "        # Разделение данных на обучающую и тестовую выборки\n",
        "        X_train, X_test, y_train, y_test = split_for_grade(X, y)\n",
        "\n",
        "\n",
        "        dim = X_train.shape[1]\n",
        "        self.indices_ = tuple(range(dim))  # Индексы всех признаков\n",
        "        self.subsets_ = [self.indices_] # Список всех подмножеств признаков\n",
        "\n",
        "        # Вычисляем R-SQUARED и MSE\n",
        "        score = self._calc_score(X_train, y_train,\n",
        "                                    X_test, y_test, self.indices_)\n",
        "        self.scores_ = [score]\n",
        "\n",
        "        while dim > self.k_features:\n",
        "            scores = []\n",
        "            subsets = []\n",
        "\n",
        "            # Перебор всех возможных подмножеств с одним удаленным признаком\n",
        "            for p in combinations(self.indices_, r=dim - 1):\n",
        "                score = self._calc_score(X_train, y_train,\n",
        "                                            X_test, y_test, p)\n",
        "                scores.append(score)\n",
        "                subsets.append(p)\n",
        "\n",
        "            # находим подмножества с лучшими значениями метрик\n",
        "            best = np.argmax([i[0] for i in scores]) #  Выбираем подмножество с наибольшим r2_score,\n",
        "                                                     # т.к. данная метрика в приоритете. Так же отбор\n",
        "                                                     # лучшей комбинации будет происходит вне класса\n",
        "            self.indices_ = subsets[best]\n",
        "            self.subsets_.append(self.indices_)\n",
        "            dim -= 1\n",
        "\n",
        "            self.scores_.append(scores[best])\n",
        "        self.k_score_ = self.scores_[-1]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Возвращает матрицу признаков с отобранными признаками.\n",
        "\n",
        "        Аргументы:\n",
        "            X: Матрица признаков.\n",
        "        \"\"\"\n",
        "        return X[:, self.indices_]\n",
        "\n",
        "    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n",
        "        \"\"\"\n",
        "        Вычисляет метрики R-SQUARED и MSE для заданного подмножества признаков.\n",
        "\n",
        "        Аргументы:\n",
        "            X_train, y_train, X_test, y_test, indices: Данные для обучения и оценки модели.\n",
        "        \"\"\"\n",
        "        self.estimator.fit(X_train[:, indices], y_train)\n",
        "        y_pred = self.estimator.predict(X_test[:, indices])\n",
        "\n",
        "        score = [r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred)]\n",
        "        return score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnsaRLc2TLur"
      },
      "source": [
        "## Инициализация моделей и главного дата фрейма"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he0gIoTCE5im"
      },
      "source": [
        "инициализация регрессионных моделей и т.п."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bzyK1PAyrxcI"
      },
      "outputs": [],
      "source": [
        "knn_model = KNeighborsRegressor(n_neighbors=3)\n",
        "scaler = MinMaxScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OUuduy4i3Sqm"
      },
      "outputs": [],
      "source": [
        "df_environmental_data = pd.read_csv(\"analysing_environmental_issues.csv\", sep=',') # главный датафрейм"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "2rgP6v2yJPbW"
      },
      "outputs": [],
      "source": [
        "#df_environmental_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCeMVlpIGV7a"
      },
      "source": [
        "## предобработка перед оценкой моделей и предсказыванием значений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZrv9db_Jk7Y"
      },
      "source": [
        "Просто копия главного датафрейма для экспериментов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "S84pSB0E3MpX"
      },
      "outputs": [],
      "source": [
        "df = df_environmental_data.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTzHo9yRKf5S"
      },
      "source": [
        "Регрессионным моделям не нравится столбец DateTime, поэтому я его пока что просто удалил, но можно поставить в качестве индекса. DateTime сам по себе не очень полезен(на этапе предобработки).   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "W5S7rikVK5-5"
      },
      "outputs": [],
      "source": [
        "df['DateTime'] = pd.to_datetime(df['DateTime'], errors='coerce')\n",
        "time_diffs = df['DateTime'].diff().dt.total_seconds()\n",
        "time_diffs = time_diffs.fillna(0)\n",
        "\n",
        "# нормализуем даты из столбца DateTime\n",
        "normalized_diffs = scaler.fit_transform(time_diffs.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "# вычисляет кумулятивную сумму элементов\n",
        "normalized_times = np.cumsum(normalized_diffs)\n",
        "\n",
        "# подставляем нормализованные значение\n",
        "df['DateTime'] = normalized_times\n",
        "\n",
        "df.pop(\"stage_4_output_danger_gas\") # значений мало, признак на данном этапе бесполезный.\n",
        "\n",
        "df = df.drop_duplicates(subset=df.columns[1:], keep=False) # удаляем все дупликаты.\n",
        "# Очень важно, что дубликаты будут найдены, если игнорировать столбец \"DateTime\".\n",
        "#df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Qlx71DJDeZ9Z"
      },
      "outputs": [],
      "source": [
        "# удаляем супер выбросы/критические ошибки, так как такие значения\n",
        "# могут непропорционально влиять на оценку параметров модели и\n",
        "# вносить нестабильность в обучение модели. Для этого используем\n",
        "# тройной межквартильный размах\n",
        "for col in df.columns[1:-1]:\n",
        "  radical_iqr_filter(df, col, fill_nan=True, multp=3)\n",
        "\n",
        "#df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYXrwAIh4Rio"
      },
      "source": [
        "Было протестированно множество разных моделей. В конченом счёте лучшими моделями оказались KNN и Random Trees Forest. В ходе размышлений было принято использовать KNN для тех признаков, если метрики R-SQUAED и MSE показали хорошие значения в тестовых выборках. Для отбора признаков в зависимости от которых будут предсказываться значения на месте пропусков в отклике."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U8Q5OYLuoDf"
      },
      "source": [
        "## **SBS  для выбора факторов , на основе которых буддет обучаться модель и предсказываться значения**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e6XG6X81SGa"
      },
      "source": [
        "**KNeighborsRegressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "84Qn0Qe1KvkN"
      },
      "outputs": [],
      "source": [
        "columns_dict_NaN = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ie-dIER4YeRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f59679c1-2981-41e6-998a-dc82ad471a66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'stage_2_output_bottom_temp': ['DateTime', 'stage_1_output_konv_avd', 'stage_2_output_bottom_pressure', 'stage_2_output_bottom_temp', 'stage_2_output_bottom_temp_hum_steam', 'stage_2_output_bottom_vacuum', 'stage_2_output_top_pressure', 'stage_2_output_top_pressure_at_end', 'stage_2_output_top_vacuum', 'stage_3_input_soft_water', 'stage_3_output_temp_hum_steam', 'stage_3_output_temp_top', 'stage_4_input_overheated_steam', 'stage_4_input_polymer', 'stage_4_input_water', 'stage_4_output_dry_residue_avg', 'stage_4_output_product'], 'stage_2_output_bottom_temp_hum_steam': ['DateTime', 'stage_1_output_konv_avd', 'stage_2_input_water_sum', 'stage_2_output_bottom_pressure', 'stage_2_output_bottom_temp', 'stage_2_output_bottom_temp_hum_steam', 'stage_2_output_bottom_vacuum', 'stage_2_output_top_pressure_at_end', 'stage_2_output_top_temp', 'stage_2_output_top_vacuum', 'stage_3_input_pressure', 'stage_3_input_soft_water', 'stage_3_input_steam', 'stage_3_output_temp_hum_steam', 'stage_3_output_temp_top', 'stage_4_input_overheated_steam', 'stage_4_input_polymer', 'stage_4_input_steam', 'stage_4_input_water', 'stage_4_output_dry_residue_avg', 'stage_4_output_product'], 'stage_2_output_bottom_vacuum': ['DateTime', 'stage_2_input_water_sum', 'stage_2_output_bottom_temp', 'stage_2_output_bottom_temp_hum_steam', 'stage_2_output_bottom_vacuum', 'stage_2_output_top_pressure', 'stage_2_output_top_pressure_at_end', 'stage_2_output_top_temp', 'stage_2_output_top_vacuum', 'stage_3_input_pressure', 'stage_3_input_soft_water', 'stage_3_input_steam', 'stage_3_output_temp_hum_steam', 'stage_3_output_temp_top', 'stage_4_input_overheated_steam', 'stage_4_input_polymer', 'stage_4_input_water', 'stage_4_output_dry_residue_avg', 'stage_4_output_product'], 'stage_2_output_top_temp': ['stage_2_input_water_sum', 'stage_2_output_bottom_pressure', 'stage_2_output_bottom_temp_hum_steam', 'stage_2_output_top_pressure', 'stage_3_input_pressure', 'stage_4_input_overheated_steam'], 'stage_2_output_top_vacuum': ['DateTime', 'stage_2_input_water_sum', 'stage_2_output_bottom_pressure', 'stage_2_output_bottom_temp_hum_steam', 'stage_2_output_bottom_vacuum', 'stage_2_output_top_pressure', 'stage_2_output_top_temp', 'stage_2_output_top_vacuum', 'stage_3_input_pressure', 'stage_3_input_steam', 'stage_3_output_temp_hum_steam', 'stage_3_output_temp_top', 'stage_4_input_overheated_steam', 'stage_4_input_polymer', 'stage_4_input_steam', 'stage_4_input_water', 'stage_4_output_product'], 'stage_3_output_temp_hum_steam': ['DateTime', 'stage_1_output_konv_avd', 'stage_2_output_bottom_pressure', 'stage_2_output_bottom_temp', 'stage_2_output_bottom_temp_hum_steam', 'stage_2_output_bottom_vacuum', 'stage_2_output_top_pressure', 'stage_2_output_top_pressure_at_end', 'stage_2_output_top_temp', 'stage_2_output_top_vacuum', 'stage_3_input_pressure', 'stage_3_input_steam', 'stage_3_output_temp_top', 'stage_4_input_overheated_steam', 'stage_4_input_polymer', 'stage_4_input_steam', 'stage_4_input_water', 'stage_4_output_dry_residue_avg', 'stage_4_output_product'], 'stage_4_input_overheated_steam': ['DateTime', 'stage_2_input_water_sum', 'stage_2_output_bottom_pressure', 'stage_2_output_bottom_temp', 'stage_2_output_bottom_vacuum', 'stage_3_input_soft_water', 'stage_3_output_temp_hum_steam', 'stage_4_input_steam', 'stage_4_output_dry_residue_avg'], 'stage_4_input_steam': ['DateTime', 'stage_1_output_konv_avd', 'stage_2_input_water_sum', 'stage_2_output_bottom_pressure', 'stage_2_output_bottom_temp', 'stage_2_output_bottom_temp_hum_steam', 'stage_2_output_bottom_vacuum', 'stage_2_output_top_pressure', 'stage_2_output_top_pressure_at_end', 'stage_2_output_top_temp', 'stage_2_output_top_vacuum', 'stage_3_input_pressure', 'stage_3_input_soft_water', 'stage_3_input_steam', 'stage_3_output_temp_hum_steam', 'stage_3_output_temp_top', 'stage_4_input_overheated_steam', 'stage_4_input_polymer', 'stage_4_input_steam', 'stage_4_input_water', 'stage_4_output_dry_residue_avg', 'stage_4_output_product']}\n"
          ]
        }
      ],
      "source": [
        "sbs = SBS(knn_model, k_features=1)\n",
        "\n",
        "for column in df.columns:\n",
        "    if df[column].isnull().any() == True:\n",
        "        y = df[column]\n",
        "        X = df.copy()\n",
        "        X.pop(column)\n",
        "\n",
        "        new_names = [i for i in range(len(df.columns))]\n",
        "        X = X.rename(columns=dict(zip(X, new_names)))\n",
        "        sbs.fit(X, y, own_split=True)\n",
        "\n",
        "        k_feat = []\n",
        "        r2 = []\n",
        "        mse = []\n",
        "\n",
        "        for scores in sbs.scores_:\n",
        "            if scores[1] <= 10 and scores[0] >= 0.9:\n",
        "                k_feat.append(len(sbs.subsets_[sbs.scores_.index(scores)]))\n",
        "                r2.append(scores[0])\n",
        "                mse.append(scores[1])\n",
        "\n",
        "        if len(mse) > 0:\n",
        "\n",
        "            r2_threshold = 0.9\n",
        "            mse_threshold = 10\n",
        "\n",
        "            # нормализация MSE\n",
        "            mse_values = np.array([item[1] for item in sbs.scores_])\n",
        "            scaler = MinMaxScaler()\n",
        "            mse_values_normalized = scaler.fit_transform(mse_values.reshape(-1, 1)).flatten()\n",
        "\n",
        "            best_r2 = -1\n",
        "            best_mse = float('inf')\n",
        "            best_pair = None\n",
        "            lk = -1\n",
        "            for i, (r2_sc, mse_sc) in enumerate(sbs.scores_):\n",
        "                normalized_mse = mse_values_normalized[i]\n",
        "                rounded_mse = round(normalized_mse, 3)\n",
        "\n",
        "                if r2_sc > r2_threshold and mse_sc <= mse_threshold:\n",
        "                    if r2_sc > best_r2 and rounded_mse < best_mse:\n",
        "                        best_r2 = r2_sc\n",
        "                        best_mse = rounded_mse\n",
        "                        best_pair = [r2_sc, mse_sc]\n",
        "                        lk = list(sbs.subsets_[sbs.scores_.index([r2_sc, mse_sc])])\n",
        "\n",
        "            #print(f\"Best count: {len(lk)}\")\n",
        "            #print(f\"Best pair: R2 = {best_pair[0]:.4f}, MSE = {best_pair[1]:.4f}, Normalized MSE = {best_mse:.4f}\")\n",
        "\n",
        "            columns_dict_NaN[column] = list(df.columns[0:][lk])\n",
        "\n",
        "print(columns_dict_NaN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tcF4gBhfZV_p"
      },
      "outputs": [],
      "source": [
        "#print(len(columns_dict_NaN.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I569V-8GvzLt"
      },
      "source": [
        "# Замена пропусков"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "db5UHjRUdF5S"
      },
      "outputs": [],
      "source": [
        "#df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL-ltfo8Gaqn"
      },
      "source": [
        "в качестве моделей, которые будут заменять пропуски в данных, будут использоваться KNN и MICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jG0YPweEUftH"
      },
      "outputs": [],
      "source": [
        "for column in columns_dict_NaN.keys(): # для каждого столбца в котором остались пропущенные значения\n",
        "\n",
        "    X = df.loc[:, columns_dict_NaN[column]].copy() # все зависимые признаки с столбцом column ()\n",
        "    y = df[column] # наш столбец.\n",
        "\n",
        "    X_train, X_test, y_train, y_temp = super_train_test_split(X, y) # Делим наши данные на\n",
        "                                                                    # на обучающую и тестовую выборку\n",
        "    knn_model.fit(X_train, y_train) # обучаем модель knn(для каждого столбца)\n",
        "\n",
        "    y_pred = knn_model.predict(X_test) # предсказываем пропущенные значения\n",
        "\n",
        "    df.loc[X_test.index, column] = y_pred # вставляем предсказания на места пропущенных значений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dTkUd832b6ol"
      },
      "outputs": [],
      "source": [
        "index_list = df.index.tolist()\n",
        "index_series = pd.Series(index_list)\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvn7S7xeiwAf"
      },
      "outputs": [],
      "source": [
        "kernel = mf.ImputationKernel(\n",
        "    data=df,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "kernel.mice(iterations=10) # Количество итерация\n",
        "\n",
        "# Получаем датафрейм без пропусков\n",
        "df = kernel.complete_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "015oLuddf0e3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "for column in df.columns[1:-3]: # для каждого столбца в котором остались пропущенные значения\n",
        "    X = df.copy() # все зависимые признаки с столбцом column ()\n",
        "    X.pop(column)\n",
        "    y = df[column] # наш столбец.\n",
        "\n",
        "    X_train, X_test, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42) # Делим наши данные на\n",
        "                                                                    # на обучающую и тестовую выборку\n",
        "    knn_model.fit(X_train, y_train) # обучаем модель knn(для каждого столбца)\n",
        "    y_pred = knn_model.predict(X_test) # предсказываем пропущенные значения\n",
        "\n",
        "    r2 = r2_score(y_temp, y_pred)\n",
        "    mse = mean_squared_error(y_temp, y_pred)\n",
        "    print(\"----------------------------\")\n",
        "    print(f'{column} - r2: {r2}, mse: {mse}')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QErSW097n-uj"
      },
      "outputs": [],
      "source": [
        "#df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp49-PfxwaCx"
      },
      "outputs": [],
      "source": [
        "df = df.set_index(index_series) # заменяем сброшенные индесы на старые для корректной работы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwgcEObp1uPJ"
      },
      "outputs": [],
      "source": [
        "# создаём фактор с значения столбца DateTime типа datetime\n",
        "date_series = pd.to_datetime(df_environmental_data['DateTime'].copy(), errors='coerce')\n",
        "#date_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xlv-RKcq0kfT"
      },
      "outputs": [],
      "source": [
        "df[\"DateTime\"] = date_series.loc[df.index] # подстановка значений типа datetime\n",
        "#df[\"DateTime\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# добавляем столб stage_4_output_danger_gas\n",
        "df[\"stage_4_output_danger_gas\"] = df_environmental_data['stage_4_output_danger_gas'].loc[df.index]\n",
        "#df[\"stage_4_output_danger_gas\"]"
      ],
      "metadata": {
        "id": "Yx6XfDFFqOfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxw2YDiYLnoZ"
      },
      "source": [
        "# Как интегрировать это чюдо в свой код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nVUUORqMQcx"
      },
      "source": [
        "Код работает, вроде все проблемные ситуации, которые нашёл, устранил. Я молодец"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPCEMrolLDw2"
      },
      "source": [
        "Насколько я понимаю, вам достаточно скопировать всё эти махинации в самое начало вашего блокнота и после последней строчки моего кода написать\n",
        "\"название вашей переменной с датафреймом\" = df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-CsLmI3LfQ7"
      },
      "outputs": [],
      "source": [
        "#my_data = df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nciUrX5SL2FI"
      },
      "source": [
        "А как буд-то мы можем себя возомнить гениями и в итоговом проекте сделать отдельный скрипт с этим кодом, который будет записывать в файлик csv Датафрейм без пропусков(или использовать данный скрипт как библиотеку и написать здесь функцию, которая будет возвращать обработанный дата фрейм), чтобы не загромождать один файл кодом."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}